"""
Analysis: Why Better Imputation Didn't Improve Model Performance
Investigating the paradox of 67% better imputation quality but comparable model results
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.impute import KNNImputer
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer

print("="*80)
print(" IMPUTATION PARADOX ANALYSIS")
print(" Why 67% Better Imputation â‰  Better Model Performance?")
print("="*80)

# Load original data with missing values
df_original = pd.read_csv('Dataset3.csv')
X_original = df_original.drop('SARSCov', axis=1)
y = df_original['SARSCov']

print("\n1. DATASET OVERVIEW")
print("="*80)
print(f"Total samples: {len(df_original)}")
print(f"Total features: {X_original.shape[1]}")
print(f"Total missing values: {X_original.isnull().sum().sum()}")
print(f"Missing percentage: {(X_original.isnull().sum().sum() / X_original.size) * 100:.2f}%")

# Missing value distribution
missing_per_feature = X_original.isnull().sum()
features_with_missing = missing_per_feature[missing_per_feature > 0]
print(f"\nFeatures with missing values: {len(features_with_missing)}")
print(f"Samples with at least one missing value: {X_original.isnull().any(axis=1).sum()} ({(X_original.isnull().any(axis=1).sum()/len(X_original))*100:.1f}%)")

# ============================================================================
# 2. APPLY BOTH IMPUTATION METHODS
# ============================================================================

print("\n2. APPLYING BOTH IMPUTATION METHODS")
print("="*80)

# KNN Imputation
print("\nApplying KNN imputation (k=5)...")
knn_imputer = KNNImputer(n_neighbors=5, weights='distance')
X_knn = pd.DataFrame(
    knn_imputer.fit_transform(X_original),
    columns=X_original.columns,
    index=X_original.index
)

# Hybrid Imputation
print("Applying Hybrid imputation...")
knn_features = ['UREA', 'PLT1', 'NET', 'MOT', 'BAT']
mice_features = [col for col in X_original.columns if col not in knn_features]

# KNN part
X_knn_temp = pd.DataFrame(
    knn_imputer.fit_transform(X_original),
    columns=X_original.columns,
    index=X_original.index
)

# MICE part
mice_imputer = IterativeImputer(max_iter=10, random_state=42)
X_mice_temp = pd.DataFrame(
    mice_imputer.fit_transform(X_original),
    columns=X_original.columns,
    index=X_original.index
)

# Combine
X_hybrid = X_original.copy()
for col in knn_features:
    if col in X_original.columns:
        X_hybrid[col] = X_knn_temp[col]
for col in mice_features:
    X_hybrid[col] = X_mice_temp[col]

print("âœ“ Both imputations complete")

# ============================================================================
# 3. ANALYZE IMPUTATION DIFFERENCES
# ============================================================================

print("\n3. ANALYZING IMPUTATION DIFFERENCES")
print("="*80)

# Calculate differences between the two imputed datasets
differences = []
for col in X_original.columns:
    if col in features_with_missing.index:
        # Only look at imputed values (where original was missing)
        mask = X_original[col].isnull()
        if mask.sum() > 0:
            knn_vals = X_knn.loc[mask, col]
            hybrid_vals = X_hybrid.loc[mask, col]
            
            diff = np.abs(knn_vals - hybrid_vals)
            mean_diff = diff.mean()
            max_diff = diff.max()
            
            differences.append({
                'Feature': col,
                'Missing_Count': mask.sum(),
                'Mean_Difference': mean_diff,
                'Max_Difference': max_diff,
                'Relative_Diff_%': (mean_diff / X_original[col].mean()) * 100 if X_original[col].mean() != 0 else 0
            })

diff_df = pd.DataFrame(differences).sort_values('Mean_Difference', ascending=False)

print("\nTop 10 Features with Largest Imputation Differences:")
print(diff_df.head(10).to_string(index=False))

print(f"\nAverage difference across all features: {diff_df['Mean_Difference'].mean():.4f}")
print(f"Average relative difference: {diff_df['Relative_Diff_%'].mean():.2f}%")

# ============================================================================
# 4. KEY INSIGHT: MISSING VALUE DISTRIBUTION
# ============================================================================

print("\n4. KEY INSIGHT: WHERE ARE THE MISSING VALUES?")
print("="*80)

# Check correlation of missing values with target
missing_by_target = []
for label in [0, 1]:
    mask = y == label
    missing_pct = (X_original[mask].isnull().sum().sum() / X_original[mask].size) * 100
    missing_by_target.append({
        'Target': 'Negative' if label == 0 else 'Positive',
        'Missing_%': missing_pct
    })

missing_target_df = pd.DataFrame(missing_by_target)
print("\nMissing Values by Target Class:")
print(missing_target_df.to_string(index=False))

# ============================================================================
# 5. CORRELATION ANALYSIS
# ============================================================================

print("\n5. CORRELATION WITH TARGET")
print("="*80)

# Calculate correlations for both imputation methods
correlations = []
for col in X_original.columns:
    knn_corr = X_knn[col].corr(y)
    hybrid_corr = X_hybrid[col].corr(y)
    
    correlations.append({
        'Feature': col,
        'KNN_Correlation': knn_corr,
        'Hybrid_Correlation': hybrid_corr,
        'Correlation_Change': hybrid_corr - knn_corr,
        'Abs_Change': abs(hybrid_corr - knn_corr)
    })

corr_df = pd.DataFrame(correlations).sort_values('Abs_Change', ascending=False)

print("\nTop 10 Features with Largest Correlation Changes:")
print(corr_df.head(10)[['Feature', 'KNN_Correlation', 'Hybrid_Correlation', 'Correlation_Change']].to_string(index=False))

print(f"\nAverage absolute correlation change: {corr_df['Abs_Change'].mean():.6f}")

# ============================================================================
# 6. THE PARADOX EXPLANATION
# ============================================================================

print("\n" + "="*80)
print(" PARADOX EXPLANATION: Why Better Imputation â‰  Better Models")
print("="*80)

print("\nðŸ“Š FINDING 1: Imputation Quality vs Model Performance")
print("-"*80)
print(f"Imputation improvement: 67% (2.20% â†’ 0.72% mean change)")
print(f"Model performance change: -0.43 pp (89.75% â†’ 89.31% AUC)")
print("\nâ†’ Better statistical preservation â‰  Better predictive signal")

print("\nðŸ“Š FINDING 2: Small Absolute Differences")
print("-"*80)
print(f"Average difference in imputed values: {diff_df['Mean_Difference'].mean():.4f}")
print(f"Average relative difference: {diff_df['Relative_Diff_%'].mean():.2f}%")
print("\nâ†’ Both methods produce similar imputed values in practice")

print("\nðŸ“Š FINDING 3: Correlation Preservation")
print("-"*80)
print(f"Average correlation change: {corr_df['Abs_Change'].mean():.6f}")
print(f"Max correlation change: {corr_df['Abs_Change'].max():.6f}")
print("\nâ†’ Both methods preserve feature-target relationships similarly")

print("\nðŸ“Š FINDING 4: Complete Cases Analysis")
print("-"*80)
complete_cases = (~X_original.isnull().any(axis=1)).sum()
print(f"Complete cases (no missing): {complete_cases} ({(complete_cases/len(X_original))*100:.1f}%)")
print(f"Cases with missing: {len(X_original) - complete_cases} ({((len(X_original) - complete_cases)/len(X_original))*100:.1f}%)")
print("\nâ†’ Only 54.6% of samples affected by imputation differences")

# ============================================================================
# 7. VISUALIZATIONS
# ============================================================================

print("\n6. GENERATING VISUALIZATIONS")
print("="*80)

fig, axes = plt.subplots(2, 2, figsize=(16, 12))

# Plot 1: Mean Change vs Model Performance
ax1 = axes[0, 0]
x_vals = [1, 2]
y1_vals = [2.20, 0.72]  # Imputation mean change
y2_vals = [89.75, 89.31]  # Model AUC

ax1_twin = ax1.twinx()
line1 = ax1.plot(x_vals, y1_vals, 'ro-', linewidth=3, markersize=10, label='Imputation Mean Change %')
line2 = ax1_twin.plot(x_vals, y2_vals, 'bo-', linewidth=3, markersize=10, label='Model AUC %')

ax1.set_xticks(x_vals)
ax1.set_xticklabels(['KNN-only', 'Hybrid'], fontsize=11)
ax1.set_ylabel('Mean Change (%)', fontsize=12, fontweight='bold', color='red')
ax1_twin.set_ylabel('Model AUC (%)', fontsize=12, fontweight='bold', color='blue')
ax1.set_title('Imputation Quality vs Model Performance\n(Paradox: Inverse Relationship)', 
              fontsize=13, fontweight='bold')
ax1.tick_params(axis='y', labelcolor='red')
ax1_twin.tick_params(axis='y', labelcolor='blue')
ax1.grid(alpha=0.3)

# Add annotations
ax1.annotate('67% improvement\nin data quality', xy=(1.5, 1.46), fontsize=10, 
             bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
ax1_twin.annotate('0.43 pp decrease\nin model AUC', xy=(1.5, 89.5), fontsize=10,
                  bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.5))

# Plot 2: Imputation Difference Distribution
ax2 = axes[0, 1]
top_diff = diff_df.head(15)
colors = plt.cm.Reds(np.linspace(0.4, 0.9, len(top_diff)))
bars = ax2.barh(range(len(top_diff)), top_diff['Mean_Difference'], color=colors, alpha=0.8)
ax2.set_yticks(range(len(top_diff)))
ax2.set_yticklabels(top_diff['Feature'], fontsize=10)
ax2.set_xlabel('Mean Absolute Difference', fontsize=12, fontweight='bold')
ax2.set_title('Imputed Value Differences (KNN vs Hybrid)\nTop 15 Features', 
              fontsize=13, fontweight='bold')
ax2.invert_yaxis()
ax2.grid(axis='x', alpha=0.3)

# Plot 3: Correlation Change
ax3 = axes[1, 0]
top_corr = corr_df.head(15)
colors_corr = ['red' if x < 0 else 'green' for x in top_corr['Correlation_Change']]
bars = ax3.barh(range(len(top_corr)), top_corr['Correlation_Change'], 
                color=colors_corr, alpha=0.7)
ax3.set_yticks(range(len(top_corr)))
ax3.set_yticklabels(top_corr['Feature'], fontsize=10)
ax3.set_xlabel('Correlation Change with Target', fontsize=12, fontweight='bold')
ax3.set_title('Feature-Target Correlation Changes\n(Hybrid minus KNN)', 
              fontsize=13, fontweight='bold')
ax3.axvline(x=0, color='black', linestyle='--', linewidth=1)
ax3.invert_yaxis()
ax3.grid(axis='x', alpha=0.3)

# Plot 4: Summary Table
ax4 = axes[1, 1]
ax4.axis('tight')
ax4.axis('off')

summary_data = [
    ['Metric', 'KNN-only', 'Hybrid', 'Change'],
    ['', '', '', ''],
    ['Imputation Quality', '', '', ''],
    ['  Mean Change %', '2.20%', '0.72%', 'â†“ 67%'],
    ['', '', '', ''],
    ['Model Performance', '', '', ''],
    ['  Best AUC', '89.75%', '89.31%', 'â†“ 0.43 pp'],
    ['  Average AUC', '82.99%', '86.34%', 'â†‘ 3.36 pp'],
    ['', '', '', ''],
    ['Key Insights', '', '', ''],
    ['  Imputation diff', f'{diff_df["Mean_Difference"].mean():.4f}', '', 'Small'],
    ['  Correlation Î”', f'{corr_df["Abs_Change"].mean():.6f}', '', 'Minimal'],
    ['  Affected samples', f'{((len(X_original) - complete_cases)/len(X_original))*100:.1f}%', '', '54.6%'],
]

table = ax4.table(cellText=summary_data, cellLoc='left', loc='center',
                  colWidths=[0.35, 0.2, 0.2, 0.25])
table.auto_set_font_size(False)
table.set_fontsize(10)
table.scale(1, 2)

# Style header row
for i in range(4):
    table[(0, i)].set_facecolor('#4472C4')
    table[(0, i)].set_text_props(weight='bold', color='white')

# Style section headers
for row in [2, 5, 9]:
    for col in range(4):
        table[(row, col)].set_facecolor('#E7E6E6')
        table[(row, col)].set_text_props(weight='bold')

ax4.set_title('Summary: Imputation Quality vs Model Performance', 
              fontsize=13, fontweight='bold', pad=20)

plt.tight_layout()
plt.savefig('covid_results_hybrid/imputation_paradox_analysis.png', dpi=300, bbox_inches='tight')
plt.close()

print("âœ“ Saved: covid_results_hybrid/imputation_paradox_analysis.png")

# ============================================================================
# 8. FINAL CONCLUSIONS
# ============================================================================

print("\n" + "="*80)
print(" CONCLUSIONS: Why Better Imputation Didn't Improve Models")
print("="*80)

print("\nðŸ” ROOT CAUSES:")
print("-"*80)

print("\n1. STATISTICAL vs PREDICTIVE QUALITY")
print("   â€¢ Imputation quality measures statistical preservation (mean, std)")
print("   â€¢ Model performance depends on predictive patterns")
print("   â€¢ These two objectives are not always aligned")

print("\n2. SMALL PRACTICAL DIFFERENCES")
print(f"   â€¢ Average imputed value difference: {diff_df['Mean_Difference'].mean():.4f}")
print(f"   â€¢ Relative to feature scale: {diff_df['Relative_Diff_%'].mean():.2f}%")
print("   â€¢ Both methods produce similar imputations for most values")

print("\n3. PRESERVED CORRELATIONS")
print(f"   â€¢ Average correlation change: {corr_df['Abs_Change'].mean():.6f}")
print("   â€¢ Feature-target relationships barely changed")
print("   â€¢ Both methods maintain predictive signal similarly")

print("\n4. LIMITED IMPACT SCOPE")
print(f"   â€¢ Only {((len(X_original) - complete_cases)/len(X_original))*100:.1f}% of samples have missing values")
print(f"   â€¢ Only {(X_original.isnull().sum().sum() / X_original.size) * 100:.2f}% of all data points imputed")
print("   â€¢ Most training data unchanged between methods")

print("\n5. MODEL ROBUSTNESS")
print("   â€¢ Tree-based models (XGBoost, RF) are robust to small value changes")
print("   â€¢ Ensemble methods average out small differences")
print("   â€¢ Stacking learns from patterns, not exact values")

print("\nðŸ’¡ KEY INSIGHT:")
print("-"*80)
print("Better imputation QUALITY â‰  Better MODEL performance because:")
print("  1. Both methods preserve predictive patterns similarly")
print("  2. Actual imputed value differences are small in practice")
print("  3. Only ~13% of data affected (rest is complete)")
print("  4. Models learn relationships, not absolute values")

print("\nâœ… RECOMMENDATIONS:")
print("-"*80)
print("1. Use HYBRID for production:")
print("   â†’ 67% better data preservation")
print("   â†’ More scientifically defensible")
print("   â†’ Minimal performance trade-off (-0.43 pp)")
print("   â†’ Better data quality = more trustworthy long-term")

print("\n2. Imputation quality matters for:")
print("   â†’ Data integrity and reproducibility")
print("   â†’ Interpretability and trust")
print("   â†’ Regulatory compliance")
print("   â†’ Not just model AUC")

print("\n3. Model performance depends on:")
print("   â†’ Signal preservation (both methods do this)")
print("   â†’ Feature engineering (same for both)")
print("   â†’ Model architecture (same for both)")
print("   â†’ Not imputation method alone")

print("\n" + "="*80)
print("âœ“ Paradox Analysis Complete!")
print("="*80)
print("\nConclusion: Better imputation quality is still valuable,")
print("even if immediate model performance gains are minimal.")
print("It ensures data integrity and trustworthiness long-term.")
